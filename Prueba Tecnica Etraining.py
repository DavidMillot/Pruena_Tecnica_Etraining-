# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c_qnvKbGbn69oH-iAJk5F_7l2pFmdioV

# PRUEBA TECNICA Etraining - Juan David Bernal Millot
# Análisis de Impacto de Precipitaciones en Ventas
## FastFood - Bogotá
## Fecha: 2/02/2026

**Contexto del Proyecto / Prueba**

Empresa: FastFood

Ubicación: Bogotá, Colombia

Objetivo: Evaluar la relación entre precipitaciones y ventas en diferentes puntos de venta

### Objetivos Específicos

1. **Extraer** datos de ventas (MySQL) y datos meteorológicos (MongoDB)
2. **Transformar** y unificar los datos para análisis
3. **Construir** un modelo predictivo que relacione precipitaciones con volumen de ventas
4. **Visualizar** patrones y resultados del análisis

### Fuentes de Datos

- **MySQL:** Base de datos transaccional (ventas, productos, tiendas)
- **MongoDB Atlas:** Sensores de precipitación en Bogotá
"""

## INSTALACION DE LIBRERIAS
!pip install pandas     ## analisis de datos
!pip install numpy      ##
!pip install pymysql    ## conexión y consultas a bases de datos MySQL
!pip install sqlalchemy ## ORM para manejar bases de datos relacionales con objetos
!pip install pymongo    ## conexión y manejo de bases de datos NoSQL MongoDB
!pip install scikit-learn   ## algoritmos de machine learning (clasificación, regresión, clustering)
!pip install matplotlib     ## visualizaciones
!pip install seaborn    ##

"""## PARTE 1: EXTRACCIÓN DE DATOS

En esta sección extraeremos datos de dos fuentes:

1. **MySQL:** Datos transaccionales de FastFood
   - Tiendas (ubicación geográfica)
   - Tickets (transacciones)
   - Ventas (detalle de productos vendidos)
   - Productos y Tipos

2. **MongoDB Atlas:** Datos meteorológicos
   - Ubicación de sensores
   - Eventos de precipitación

---
"""

import pandas as pd
from sqlalchemy import create_engine

# Credenciales MySQL
user = "user1"
password = "AVNS_FB2xcAz2en7pG0lHIsS"
host = "mysql-3b4106d0-etraining-62c0.g.aivencloud.com"
port = "10185"
database = "FastFood"

engine = create_engine(
    f"mysql+pymysql://{user}:{password}@{host}:{port}/{database}"
)

# Carga de tablas
df_product  = pd.read_sql("SELECT * FROM Product;", engine)
df_region   = pd.read_sql("SELECT * FROM Region;", engine)
df_size     = pd.read_sql("SELECT * FROM Size;", engine)
df_ticket   = pd.read_sql("SELECT * FROM Ticket;", engine)
df_tiendas  = pd.read_sql("SELECT * FROM Tiendas;", engine)
df_type     = pd.read_sql("SELECT * FROM Type;", engine)
df_ventas   = pd.read_sql("SELECT * FROM Ventas;", engine)

df_tiendas.head()

## Conexión y extracción de datos – MongoDB Atlas
from pymongo import MongoClient

mongo_user = "user1"
mongo_pass = "6SG5pdEUZGHbZwWC"

mongo_uri = (
    f"mongodb+srv://{mongo_user}:{mongo_pass}"
    "@cluster0.9ytpxrr.mongodb.net/?retryWrites=true&w=majority"
)

client = MongoClient(mongo_uri)

# Base correcta confirmada por el evaluador
db = client["test"]

db.list_collection_names()

# Carga de colecciones
df_sensores = pd.DataFrame(list(db["Ubicacion_sensores"].find()))
df_eventos  = pd.DataFrame(list(db["sensor_eventos"].find()))

df_sensores.head(), df_eventos.head()

"""## PARTE 2:

1. Limpieza y transformación (ETL)
---
"""

df_sensores_clean = df_sensores.copy()

df_sensores_clean[["latitud", "longitud"]] = (
    df_sensores_clean["ubicacion"]
    .str.split(",", expand=True)
    .apply(lambda x: x.str.strip())
)

df_sensores_clean["latitud"] = pd.to_numeric(df_sensores_clean["latitud"])
df_sensores_clean["longitud"] = pd.to_numeric(df_sensores_clean["longitud"])

"""2.1 Eventos de sensores – parsing correcto
---
"""

df_eventos_clean = df_eventos.copy()

# Identificar columna concatenada de forma robusta
col_eventos = [c for c in df_eventos_clean.columns if ";" in c][0]

df_eventos_clean[["id", "sensor_id", "valor", "fecha"]] = (
    df_eventos_clean[col_eventos].str.split(";", expand=True)
)

df_eventos_clean["sensor_id"] = pd.to_numeric(df_eventos_clean["sensor_id"])
df_eventos_clean["valor"] = pd.to_numeric(df_eventos_clean["valor"])
df_eventos_clean["fecha"] = pd.to_datetime(df_eventos_clean["fecha"])

"""2.2 Datos diarios de lluvia
---
"""

df_eventos_daily = (
    df_eventos_clean
    .assign(fecha_dia=lambda x: x["fecha"].dt.date)
    .groupby(["sensor_id", "fecha_dia"], as_index=False)
    .agg(lluvia_mm=("valor", "sum"))
)

"""2.3 Limpieza de tiendas (coordenadas)
---
"""

df_tiendas_clean = df_tiendas.copy()

df_tiendas_clean[["latitud", "longitud"]] = (
    df_tiendas_clean["ubicacion"]
    .str.split(",", expand=True)
    .apply(lambda x: x.str.strip())
)

df_tiendas_clean["latitud"] = pd.to_numeric(df_tiendas_clean["latitud"], errors="coerce")
df_tiendas_clean["longitud"] = pd.to_numeric(df_tiendas_clean["longitud"], errors="coerce")

"""2.4 Ventas"""

## Renombrar IDs para evitar colisiones
ticket  = df_ticket.rename(columns={"id": "ticket_id"})
product = df_product.rename(columns={"id": "product_id_ref"})
tipo    = df_type.rename(columns={"id": "tipo_compra_id_ref"})
tiendas = df_tiendas_clean.rename(columns={"id": "tienda_id_ref"})

## Renombrar IDs para evitar colisiones
df_ventas_base = df_ventas.merge(
    ticket,
    left_on="factura_id",
    right_on="ticket_id",
    how="inner"
)

## Renombrar IDs para evitar colisiones
df_ventas_base = df_ventas.merge(
    ticket,
    left_on="factura_id",
    right_on="ticket_id",
    how="inner"
)

## Renombrar IDs para evitar colisiones
df_ventas_base = df_ventas_base.merge(
    product,
    left_on="product_id",
    right_on="product_id_ref",
    how="left"
)

## Renombrar IDs para evitar colisiones
df_ventas_base = df_ventas_base.merge(
    tipo,
    left_on="tipo_compra_id",
    right_on="tipo_compra_id_ref",
    how="left"
)

## Renombrar IDs para evitar colisiones
df_ventas_base = df_ventas_base.merge(
    tiendas,
    left_on="tienda_id",
    right_on="tienda_id_ref",
    how="left"
)

## CHEQUEO DEL DATA FINAL
df_ticket.columns

df_ventas_base.head()

"""## 3. Sensor más cercano por tienda (robusto y reproducible)"""

import numpy as np
import pandas as pd

# Aseguramos tipos
df_tiendas_geo = (
    df_tiendas_clean
    .dropna(subset=["latitud", "longitud"])
    [["id", "latitud", "longitud"]]
    .rename(columns={"id": "tienda_id"})
)

df_sensores_geo = (
    df_sensores_clean
    [["id", "latitud", "longitud"]]
    .rename(columns={"id": "sensor_id"})
)

"""3.1 Haversine (ecuación matemática utilizada en navegación y geolocalización para calcular la distancia más corta (distancia ortodrómica) entre dos puntos sobre la superficie de una esfera, como la Tierra, conociendo sus latitudes y longitudes.)"""

def haversine(lat1, lon1, lat2, lon2):
    R = 6371.0
    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])
    dlat = lat2 - lat1
    dlon = lon2 - lon1
    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2
    return 2 * R * np.arcsin(np.sqrt(a))

"""3.2 Calcular sensor más cercano"""

pairs = []
for _, t in df_tiendas_geo.iterrows():
    for _, s in df_sensores_geo.iterrows():
        pairs.append([
            t["tienda_id"],
            s["sensor_id"],
            haversine(t["latitud"], t["longitud"], s["latitud"], s["longitud"])
        ])

df_dist = pd.DataFrame(pairs, columns=["tienda_id", "sensor_id", "dist_km"])

df_sensor_cercano = (
    df_dist.sort_values("dist_km")
    .groupby("tienda_id", as_index=False)
    .first()
)

df_sensor_cercano.head()

"""## 4. Lluvia diaria por sensor (ya corregida)

Clave técnica: los sensores están por hora; las ventas por fecha. Agregamos lluvia diaria.
"""

df_eventos_daily = (
    df_eventos_clean
    .assign(fecha_dia=lambda x: x["fecha"].dt.date)
    .groupby(["sensor_id", "fecha_dia"], as_index=False)
    .agg(lluvia_mm=("valor", "sum"))
)

df_eventos_daily.head()

"""## 5. Dataset final para modelo"""

# Añadimos sensor cercano
df_modelo = df_ventas_base.merge(
    df_sensor_cercano,
    on="tienda_id",
    how="left"
)

# Fecha a día
df_modelo["fecha_dia"] = pd.to_datetime(df_modelo["fecha_venta"]).dt.date

# Unimos lluvia
df_modelo = df_modelo.merge(
    df_eventos_daily,
    on=["sensor_id", "fecha_dia"],
    how="left"
)

df_modelo.head()

"""## ANALISIS DE DATOS Y ESTADISTICA DESCRIPTIVA"""

df_modelo.shape ## Dimensión del dataset

list(df_modelo.columns) ## Variables y sus respectivos nombres

df_modelo.dtypes ## Estructura y tipos de variables

## Calidad de datos: valores faltantes (NA)
na_summary = (
    df_modelo.isna()
    .sum()
    .reset_index()
    .rename(columns={"index": "variable", 0: "n_NA"})
)

na_summary["porcentaje_NA"] = 100 * na_summary["n_NA"] / len(df_modelo)
na_summary.sort_values("porcentaje_NA", ascending=False)

# Variables numéricas
num_vars = df_modelo.select_dtypes(include=["number"]).columns.tolist()

# Variables categóricas
cat_vars = df_modelo.select_dtypes(include=["object"]).columns.tolist()

# Variables temporales
date_vars = df_modelo.select_dtypes(include=["datetime"]).columns.tolist()

num_vars, cat_vars, date_vars

df_modelo[num_vars].describe().T

df_modelo[num_vars].agg(
    ["mean", "std", "var", "min", "max"]
).T

outlier_summary = {}

for v in num_vars:
    q1 = df_modelo[v].quantile(0.25)
    q3 = df_modelo[v].quantile(0.75)
    iqr = q3 - q1
    outliers = ((df_modelo[v] < q1 - 1.5 * iqr) | (df_modelo[v] > q3 + 1.5 * iqr)).sum()
    outlier_summary[v] = outliers

pd.DataFrame.from_dict(outlier_summary, orient="index", columns=["n_outliers"])

for v in cat_vars:
    print(f"\nVariable: {v}")
    print("Número de categorías:", df_modelo[v].nunique())
    print(df_modelo[v].value_counts().head())

"""CONCLUSIÓN DESCRIPTIVA

El análisis descriptivo permitió caracterizar exhaustivamente la base de datos integrada, identificando su dimensión, estructura, calidad y comportamiento estadístico.
Se evidenció una alta heterogeneidad en las variables categóricas, especialmente en el tipo de producto, así como una elevada variabilidad en las variables numéricas asociadas a ventas y precipitación.
El control de valores faltantes y la detección de posibles valores atípicos permitió validar la consistencia del dataset antes del modelado predictivo, garantizando la trazabilidad y confiabilidad del proceso ETL.
"""



df_y.describe().T

df_y["rango_lluvia"] = pd.qcut(
    df_y["lluvia_mm"], q=4, duplicates="drop"
)

df_y.groupby("rango_lluvia")["ventas"].agg(
    ["count", "mean", "std"]
)

na_df = (
    df_modelo.isna()
    .sum()
    .to_frame(name="n_NA")
)

na_df["porcentaje_NA"] = 100 * na_df["n_NA"] / len(df_modelo)
na_df.sort_values("porcentaje_NA", ascending=False)

"""## 6. Variable objetivo (ventas) y features

Como no hay cantidad en Ticket, modelamos conteo de tickets por tienda-día (volumen de ventas).
"""

df_y = (
    df_modelo
    .groupby(["tienda_id", "fecha_dia"], as_index=False)
    .agg(
        ventas=("ticket_id", "count"),
        lluvia_mm=("lluvia_mm", "first")
    )
    .dropna(subset=["lluvia_mm"])
)

df_y.head()

"""## Modelado predictivo

 Regresión Lineal
"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_absolute_error

X = df_y[["lluvia_mm"]]
y = df_y["ventas"]

modelo = LinearRegression()
modelo.fit(X, y)

y_pred = modelo.predict(X)

print(f"R2: {r2_score(y, y_pred):.3f}")
print(f"MAE: {mean_absolute_error(y, y_pred):.3f}")
print(f"Coeficiente lluvia: {modelo.coef_[0]:.3f}")
print(f"Intercepto: {modelo.intercept_:.3f}")

import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
plt.scatter(df_y["lluvia_mm"], df_y["ventas"], alpha=0.6)
plt.plot(df_y["lluvia_mm"], y_pred, color="red", linewidth=2)
plt.xlabel("Precipitación diaria (mm)")
plt.ylabel("Volumen de ventas (tickets por día)")
plt.title("Impacto de la lluvia en el volumen de ventas")
plt.show()

"""**RESULTADOS del modelo de regresión lineal**

**Calidad del (R² = 0.271)**

El coeficiente de determinación/Chi-cuadradp R² = 0.271 indica que aproximadamente el 27% de la variabilidad en el volumen diario de ventas (medido como número de tickets) puede ser explicada por la precipitación diaria.

Este valor se considera moderado, lo cual es esperable en un contexto real, ya que el comportamiento de ventas está influenciado por múltiples factores adicionales no incluidos en el modelo (día de la semana, horarios, promociones, tipo de producto, ubicación, entre otros como variables exogenas).

**Error del modelo (MAE = 23.656)**

El MAE ≈ 23.7 indica que, en promedio, el modelo se equivoca en alrededor de 24 tickets por día al predecir el volumen de ventas.

Dado que los valores observados de ventas diarias oscilan aproximadamente entre 20 y 150 tickets, este error es consistente con un modelo base que utiliza una sola variable explicativa.

**Coeficiente de la lluvia (β = 0.007)**

Esto puede interpretarse de la siguiente forma:

***Cada incremento de 1.000 mm en la precipitación diaria se asocia, en promedio, con un aumento aproximado de 7 tickets diarios.***

Este resultado sugiere que, en este contexto específico, la lluvia podría estar incentivando pedidos en línea o a domicilio, compensando una posible disminución en ventas presenciales.
"""

df_modelo.shape

ruta = "/content/df_modelo_fastfood.xlsx"
df_modelo.to_excel(ruta, index=False)

print("Archivo creado en:", ruta)

from google.colab import files
files.download(ruta)